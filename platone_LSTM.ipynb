{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schmutzz/PV_Forecast_Platone/blob/master/platone_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MBUUEQIAdvym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5GOJwunl_oP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, concatenate\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# establish sql connection\n",
        "db_path = \"drive/MyDrive/data/input_data.db\"\n",
        "conn = sqlite3.connect(db_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "parser = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "df_station43_long = pd.read_sql_query('SELECT Date, solar_radiation, pressureTrend, windspeedAvg, avg_wind_dir '\n",
        "                                      'FROM wunderground_historical_43_long', conn,\n",
        "                                      parse_dates=['Date'], index_col='Date')\n",
        "df_station43_long.dropna(inplace=True)\n",
        "# df_station43_long = df_station43_long.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "df_mb_15 = pd.read_sql_query('SELECT Timestamp, pvpower_instant FROM mb_pvpro_15min', conn, \n",
        "                             parse_dates=['Timestamp'], index_col='Timestamp')\n",
        "df_mb_15.dropna(inplace=True)\n",
        "df_mb_15 = df_mb_15.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "df_mb_clouds = pd.read_sql_query('SELECT Timestamp, lowclouds, midclouds, highclouds, totalcloudcover '\n",
        "                                 'FROM mb_clouds', conn, parse_dates=['Timestamp'], index_col='Timestamp')\n",
        "df_mb_clouds.dropna(inplace=True)\n",
        "df_mb_clouds = df_mb_clouds.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "# convert dtypes to reduce RAM usage\n",
        "df_station43_long['solar_radiation'] = df_station43_long['solar_radiation'].astype(np.float32)\n",
        "df_station43_long['avg_wind_dir'] = df_station43_long['avg_wind_dir'].astype(np.int16)\n",
        "df_station43_long['windspeedAvg'] = df_station43_long['windspeedAvg'].astype(np.int8)\n",
        "# df_station43_long['tempAvg'] = df_station43_long['tempAvg'].astype(np.int8)\n",
        "df_station43_long['pressureTrend'] = df_station43_long['pressureTrend'].astype(np.float16)\n",
        "df_station43_long['day'] = df_station43_long.index.day.astype(np.int8)\n",
        "df_station43_long['month'] = df_station43_long.index.month.astype(np.int8)\n",
        "df_station43_long['hour'] = df_station43_long.index.hour.astype(np.int8)\n",
        "df_station43_long['minute'] = df_station43_long.index.minute.astype(np.int8)\n",
        "# df_station43_long['precipTotal'] = df_station43_long['precipTotal'].astype(np.float16)"
      ],
      "metadata": {
        "id": "NAICyFTBcwlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# fig = px.scatter(df_station43_long, y='solar_radiation')\n",
        "# fig.show()"
      ],
      "metadata": {
        "id": "U4AZdozNcwly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "handle missing data (01.01.2021-09.05.2021 and 26.05.2021-29.11.2022)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ZuTIfqYYcwl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_station43_long = df_station43_long.resample('5Min').mean().interpolate(method='linear')"
      ],
      "metadata": {
        "id": "sVe7dlEfEEIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# interpolate small data gaps\n",
        "# data_1 = df_station43_long.loc[:pd.to_datetime('09.05.2021', format='%d.%m.%Y')]\n",
        "data_2 = df_station43_long.merge(df_mb_15, how='inner', left_index=True, right_index=True)\n",
        "data_2 = data_2.merge(df_mb_clouds, how='inner', left_index=True, right_index=True)\n",
        "\n",
        "# remove negative values for scaling\n",
        "data_2.loc[data_2['solar_radiation'] < 0, 'solar_radiation'] = 0\n",
        "data_2.loc[data_2['pvpower_instant'] < 0, 'pvpower_instant'] = 0\n",
        "\n",
        "# del df_station43_long\n",
        "\n",
        "# data_1 = data_1.resample('5Min').mean().interpolate(method='quadratic')\n",
        "# data_2 = data_2.resample('5Min').mean().interpolate(method='quadratic')\n",
        "\n",
        "# validation split ~10%\n",
        "\"\"\"split_1 = pd.to_datetime('20.04.2021', format='%d.%m.%Y')\n",
        "split_2 = pd.to_datetime('10.10.2022', format='%d.%m.%Y')\n",
        "\n",
        "train_1 = data_1.loc[:split_1]\n",
        "test_1 = data_1.loc[split_1:]\n",
        "\n",
        "train_2 = data_2.loc[:split_2]\n",
        "test_2 = data_2.loc[split_2:]\n",
        "\n",
        "del data_1, data_2\"\"\""
      ],
      "metadata": {
        "id": "ei16UJtKcwl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test = data_1.to_numpy().T[0].reshape(-1, 1)\n",
        "# data_1.shape[-1]"
      ],
      "metadata": {
        "id": "NZxyEgiW4Lx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# scale the data using MinMax Scaler from to 1 as LSTM has a default tanh activation function\n",
        "# use data_2 for scaling since the majority of dates are in the second training set\n",
        "# scaler = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy())\n",
        "\n",
        "# scalers_1 = [MinMaxScaler(feature_range=(-1,1)).fit(data_1.to_numpy().T[i].reshape(-1, 1)) for i in range(data_1.shape[-1])]\n",
        "scalers_2 = [MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy().T[i].reshape(-1, 1)) for i in range(data_2.shape[-1])]\n",
        "\n",
        "# data_1_scaled = np.array([scaler.transform(data_1.to_numpy().T[i].reshape(-1, 1)) for i, scaler in enumerate(scalers_1)]).squeeze()\n",
        "data_2_scaled = np.array([scaler.transform(data_2.to_numpy().T[i].reshape(-1, 1)) for i, scaler in enumerate(scalers_2)]).squeeze()\n",
        "\n",
        "\"\"\"\n",
        "scaler_solar = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[0].reshape(-1, 1))\n",
        "scaler_pressure = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[1].reshape(-1, 1))\n",
        "scaler_day = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[2].reshape(-1, 1))\n",
        "scaler_month = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[3].reshape(-1, 1))\n",
        "scaler_hour = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[4].reshape(-1, 1))\n",
        "scaler_minute = MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy()[5].reshape(-1, 1))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "test_scaled = scaler.transform(test_1.to_numpy())\n",
        "train_2_scaled = scaler.transform(train_2.to_numpy())\n",
        "test_2_scaled = scaler.transform(test_2.to_numpy())\n",
        "del train_1, test_1, train_2, test_2\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SzNlD5xCcwl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create a function to split the datasets into two week windows\n",
        "timesteps_input = 12*24  # 12 five min intervals * 24 hours = 1 day\n",
        "timesteps_prediction = 12*6  # 12 five min intervals * 2 hours\n",
        "\n",
        "def create_dataset(dataset, steps_in=timesteps_input, steps_pred=timesteps_prediction):\n",
        "    \"\"\"\n",
        "    Function which creates two week chunks of x_train data, and a single\n",
        "    value for y_train.\n",
        "    \"\"\"\n",
        "    X_hist, X_pred, y = [], [], []\n",
        "    print(dataset.shape)\n",
        "    for i in tqdm(range(dataset.shape[1])):\n",
        "        target_val_start = i + steps_in\n",
        "        target_val_end = target_val_start + steps_pred\n",
        "        if target_val_end >= dataset.shape[1]:\n",
        "            break\n",
        "        feature_chunk, meteoblue_pred, target = dataset[:-5, i:target_val_start], \\\n",
        "                                                dataset[-5:, target_val_start:target_val_end], \\\n",
        "                                                dataset[0, target_val_start:target_val_end]\n",
        "        X_hist.append(feature_chunk)\n",
        "        X_pred.append(meteoblue_pred)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X_hist), np.array(X_pred), np.array(y)"
      ],
      "metadata": {
        "id": "ZfCNC6cNcwl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "test1 = np.zeros((5, 10))\n",
        "test2 = np.ones((3, 5))\n",
        "np.append(test1, test2)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "h7mkx597lGZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create training data for NN\n",
        "# X_1, y_1 = create_dataset(data_1_scaled)\n",
        "X_hist_2, X_pred_2, y_2 = create_dataset(data_2_scaled)\n",
        "# del train_1_scaled, train_2_scaled\n",
        "\n",
        "# combine all training data\n",
        "# save data to dataset 2 for RAM reasons\n",
        "\"\"\"\n",
        "X_train_2 = np.append(X_train_1, X_train_2, axis=0)\n",
        "del X_train_1\n",
        "y_train_2 = np.append(y_train_1, y_train_2, axis=0)\n",
        "del y_train_1\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zcHHqiCCcwl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# create testing data for NN\n",
        "X_test_1, y_test_1 = create_dataset(test_1_scaled)\n",
        "X_test_2, y_test_2 = create_dataset(test_2_scaled)\n",
        "# del test_1_scaled, test_2_scaled\n",
        "\n",
        "# combine all test data\n",
        "X_test_2 = np.append(X_test_1, X_test_2, axis=0)\n",
        "del X_test_1\n",
        "y_test_2 = np.append(y_test_1, y_test_2, axis=0)\n",
        "del y_test_1\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1GjzfDBVcwl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X_train, X_test, y_train, y_test = X_train_2, X_test_2, y_train_2, y_test_2\n",
        "del X_train_2, X_test_2, y_train_2, y_test_2\n",
        "\"\"\"\n",
        "# print(X_1.shape)\n",
        "# print(y_1.shape)\n",
        "print(X_hist_2.shape)\n",
        "print(X_pred_2.shape)\n",
        "print(y_2.shape)"
      ],
      "metadata": {
        "id": "vE1cJ1fKfCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create LSTM Model"
      ],
      "metadata": {
        "id": "T9FM8kGKejQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input needs to be [samples, timesteps, features]\n",
        "units_1 = 96\n",
        "units_2 = 64\n",
        "units_3 = 64\n",
        "units_dense = 32\n",
        "dropout = 0.05\n",
        "epochs = 40\n",
        "val_split = 0.1\n",
        "optimizer = 'adam'\n",
        "file_name = 'wg_multi_6hour_branches_mb_forecast_L96rs_L96_D64_C_D64_better_scaling'\n",
        "\n",
        "# multiple inputs from https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
        "input_hist = Input(shape=(X_hist_2.shape[1], X_hist_2.shape[2]))\n",
        "input_pred = Input(shape=(X_pred_2.shape[1], X_pred_2.shape[2]))\n",
        "\n",
        "# branch for historical wonderground data\n",
        "x = LSTM(96, dropout=dropout, return_sequences=True)(input_hist)\n",
        "x = LSTM(96)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Model(inputs=input_hist, outputs=x)\n",
        "\n",
        "# branch for meteoblue forecast data\n",
        "y = LSTM(96, dropout=dropout, return_sequences=True)(input_pred)\n",
        "y = LSTM(96)(y)\n",
        "y = Dense(64, activation='relu')(y)\n",
        "y = Model(inputs=input_pred, outputs=y)\n",
        "\n",
        "# combine branches\n",
        "combined = concatenate([x.output, y.output])\n",
        "\n",
        "z = Dense(64, activation='relu')(combined)\n",
        "# z = LSTM(64)(z)\n",
        "z = Dense(y_2.shape[1])(z)\n",
        "model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "\n",
        "checkpoint_filepath = 'drive/MyDrive/data/{}_cp'.format(file_name)\n",
        "mcp_save = keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
        "early_stopping = EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)\n",
        "\n",
        "\"\"\"\n",
        "model = keras.Sequential()\n",
        "model.add(LSTM(units=units_1, dropout=dropout, return_sequences=True,\n",
        "                input_shape=(X_hist_2.shape[1], X_hist_2.shape[2])))\n",
        "model.add(LSTM(units=units_2, dropout=dropout, return_sequences=True))\n",
        "model.add(LSTM(units=units_3, dropout=dropout))\n",
        "model.add(Dense(units=units_dense))\n",
        "model.add(Dense(units=y_2.shape[1]))\n",
        "\"\"\"\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "print(model.summary())\n",
        "\n",
        "# history_1 = model.fit(X_1, y_1, validation_split=val_split, batch_size=192,\n",
        "#             epochs=epochs, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "history_2 = model.fit([X_hist_2, X_pred_2], y_2, validation_split=val_split, batch_size=96,\n",
        "            epochs=epochs, verbose=1, callbacks=[mcp_save])\n",
        "\n",
        "# batchsize größer"
      ],
      "metadata": {
        "id": "rYUQJ_vxeqrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history_2.history[\"loss\"]\n",
        "val_loss = history_2.history[\"val_loss\"]\n",
        "epoch = np.arange(1, len(val_loss)+1, 1)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "plt.plot(epoch,loss)\n",
        "plt.plot(epoch,val_loss)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Z32Y8CbjZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('drive/MyDrive/data/{}'.format(file_name))"
      ],
      "metadata": {
        "id": "qbiI5z0yqPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model('drive/MyDrive/data/{}'.format(file_name))"
      ],
      "metadata": {
        "id": "AwuoU67VxAin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot relevant times (7am - 3pm)\n",
        "\n",
        "while True:\n",
        "    r = random.randrange(0, len(X_hist_2))\n",
        "    x_hist = X_hist_2[r]\n",
        "    x_hist_exp = np.expand_dims(x_hist, 0)\n",
        "    x_pred = X_pred_2[r]\n",
        "    x_pred_exp = np.expand_dims(x_pred, 0)\n",
        "    expected = y_2[r]\n",
        "    prediction = model([x_hist_exp, x_pred_exp], training=False)[0]\n",
        "\n",
        "    x_hist_real = [scaler.inverse_transform(x_hist[i].reshape(-1, 1)) for i, scaler in enumerate(scalers_2[:-5])]\n",
        "    x_pred_real = [scaler.inverse_transform(x_pred[i].reshape(-1, 1)) for i, scaler in enumerate(scalers_2[-5:])]\n",
        "    expected_real = scalers_2[0].inverse_transform(expected.reshape(-1, 1))\n",
        "    prediction_real = scalers_2[0].inverse_transform(prediction.numpy().reshape(-1, 1))\n",
        "\n",
        "    if int(x_hist_real[-2][-1]) in range(7, 15, 1):\n",
        "        break\n",
        "\n",
        "\"\"\"\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=('Scaled', 'Real'))\n",
        "\n",
        "fig.add_trace(go.Scatter(y=expected, name='expected'), 1, 1)\n",
        "fig.add_trace(go.Scatter(y=prediction, name='prediction'), 1, 1)\n",
        "fig.add_trace(go.Scatter(y=expected_real.flatten(), name='expected'), 1, 2)\n",
        "fig.add_trace(go.Scatter(y=prediction_real.flatten(), name='prediction'), 1, 2)\n",
        "fig.show()\n",
        "\"\"\"\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 9))\n",
        "# fig = plt.figure(figsize=(12, 8))\n",
        "axes[0].set_title('Scaled Values - Date: {:02d}.-{:02d}. {:02d}:{:02d}'.format(int(x_hist_real[-4][-1]), int(x_hist_real[-3][-1]), \n",
        "                                                       int(x_hist_real[-2][-1]), int(x_hist_real[-1][-1])))\n",
        "axes[0].plot(expected, label='Expected')\n",
        "axes[0].plot(prediction, label='Prediction')\n",
        "axes[0].plot(x_pred[0], label='MeteoBlue PV-Forecast')\n",
        "axes[0].plot(x_pred[-1], label='MeteoBlue Cloud-Forecast')\n",
        "axes[0].legend()\n",
        "\n",
        "print('MSE MeteoBlue: {:.4f}'.format(mean_squared_error(expected, x_pred[0])))\n",
        "print('MSE Prediction: {:.4f}'.format(mean_squared_error(expected, prediction)))\n",
        "print('\\n')\n",
        "\n",
        "# fig = plt.figure(figsize=(12, 8))\n",
        "axes[1].set_title('Real Values - Date: {:02d}.-{:02d}. {:02d}:{:02d}'.format(int(x_hist_real[-4][-1]), int(x_hist_real[-3][-1]), \n",
        "                                                       int(x_hist_real[-2][-1]), int(x_hist_real[-1][-1])))\n",
        "axes[1].plot(expected_real, label='Expected')\n",
        "axes[1].plot(prediction_real, label='Prediction')\n",
        "axes[1].plot(x_pred_real[0], label='MeteoBlue Forecast')\n",
        "# axes[1].plot(x_pred_real[-1], label='MeteoBlue Cloud-Forecast')\n",
        "axes[1].legend()\n",
        "\n",
        "print('MSE MeteoBlue: {:.4f}'.format(mean_squared_error(expected_real, x_pred_real[0])))\n",
        "print('MSE Prediction: {:.4f}'.format(mean_squared_error(expected_real, prediction_real)))\n",
        "print('\\n')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "wmw2Kevc-mJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2.min()"
      ],
      "metadata": {
        "id": "qJZN7TdvCePS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2.max()"
      ],
      "metadata": {
        "id": "Z0Ci5QjAH2jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mZOzmN_DwDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73-iYOEIIEEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
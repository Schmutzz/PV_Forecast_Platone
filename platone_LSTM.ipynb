{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MBUUEQIAdvym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5GOJwunl_oP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, concatenate, Activation\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "# establish sql connection\n",
        "db_path = \"drive/MyDrive/data/input_data.db\"\n",
        "conn = sqlite3.connect(db_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "parser = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "df_station43_long = pd.read_sql_query('SELECT Date, solar_radiation, pressureTrend, windspeedAvg, avg_wind_dir '\n",
        "                                      'FROM wunderground_historical_43_long', conn,\n",
        "                                      parse_dates=['Date'], index_col='Date')\n",
        "df_station43_long.dropna(inplace=True)\n",
        "# df_station43_long = df_station43_long.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "df_mb_15 = pd.read_sql_query('SELECT Timestamp, pvpower_instant FROM mb_pvpro_15min', conn, \n",
        "                             parse_dates=['Timestamp'], index_col='Timestamp')\n",
        "df_mb_15.dropna(inplace=True)\n",
        "df_mb_15 = df_mb_15.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "df_mb_60 = pd.read_sql_query('SELECT Timestamp, pvpower_instant FROM mb_pvpro_1h', conn, \n",
        "                             parse_dates=['Timestamp'], index_col='Timestamp')\n",
        "df_mb_60.dropna(inplace=True)\n",
        "df_mb_60 = df_mb_60.resample('5Min').mean().interpolate(method='linear')\n",
        "df_mb_pv = pd.concat([df_mb_60, df_mb_15])\n",
        "\n",
        "df_mb_clouds = pd.read_sql_query('SELECT Timestamp, lowclouds, midclouds, highclouds, totalcloudcover '\n",
        "                                 'FROM mb_clouds', conn, parse_dates=['Timestamp'], index_col='Timestamp')\n",
        "df_mb_clouds.dropna(inplace=True)\n",
        "df_mb_clouds = df_mb_clouds.resample('5Min').mean().interpolate(method='linear')\n",
        "\n",
        "# convert dtypes to reduce RAM usage\n",
        "df_station43_long['solar_radiation'] = df_station43_long['solar_radiation'].astype(np.float32)\n",
        "df_station43_long['avg_wind_dir'] = df_station43_long['avg_wind_dir'].astype(np.int16)\n",
        "df_station43_long['windspeedAvg'] = df_station43_long['windspeedAvg'].astype(np.int8)\n",
        "df_station43_long['pressureTrend'] = df_station43_long['pressureTrend'].astype(np.float16)\n"
      ],
      "metadata": {
        "id": "NAICyFTBcwlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_station43_long = df_station43_long.resample('5Min').mean().interpolate(method='quadratic')\n",
        "\n",
        "df_station43_long['day'] = df_station43_long.index.day.astype(np.int8)\n",
        "df_station43_long['month'] = df_station43_long.index.month.astype(np.int8)\n",
        "df_station43_long['hour'] = df_station43_long.index.hour.astype(np.int8)\n",
        "df_station43_long['minute'] = df_station43_long.index.minute.astype(np.int8)"
      ],
      "metadata": {
        "id": "sVe7dlEfEEIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "handle missing data (09.05.2021 - 26.05.2021)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ZuTIfqYYcwl0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# interpolate small data gaps\n",
        "split_date = pd.to_datetime('25.09.2022', format='%d.%m.%Y')\n",
        "\n",
        "data_1 = df_station43_long.loc[split_date:]\n",
        "data_2 = df_station43_long.loc[pd.to_datetime('26.05.2021', format='%d.%m.%Y'):split_date]\n",
        "\n",
        "# change structure so date values are always in the first four spots\n",
        "cols = data_1.columns.tolist()\n",
        "cols = cols[-4:] + cols[:-4]\n",
        "data_1 = data_1[cols]\n",
        "data_2 = data_2[cols]\n",
        "\n",
        "# merge forecast data\n",
        "data_1 = data_1.merge(df_mb_pv, how='inner', left_index=True, right_index=True)\n",
        "data_1 = data_1.merge(df_mb_clouds, how='inner', left_index=True, right_index=True)\n",
        "\n",
        "data_2 = data_2.merge(df_mb_pv, how='inner', left_index=True, right_index=True)\n",
        "data_2 = data_2.merge(df_mb_clouds, how='inner', left_index=True, right_index=True)\n",
        "\n",
        "# remove negative values for scaling\n",
        "data_2.loc[data_2['solar_radiation'] < 0, 'solar_radiation'] = 0\n",
        "data_2.loc[data_2['pvpower_instant'] < 0, 'pvpower_instant'] = 0\n",
        "\n",
        "data_1.loc[data_1['solar_radiation'] < 0, 'solar_radiation'] = 0\n",
        "data_1.loc[data_1['pvpower_instant'] < 0, 'pvpower_instant'] = 0\n",
        "\n",
        "# maximum solar radiation should be 1000\n",
        "# print(data_1.max())\n",
        "data_1.loc[data_1['solar_radiation'] > 0, 'solar_radiation'] = 1000\n",
        "data_2.loc[data_2['solar_radiation'] > 0, 'solar_radiation'] = 1000"
      ],
      "metadata": {
        "id": "ei16UJtKcwl2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# scale the data using MinMax Scaler from to 1 as LSTM has a default tanh activation function\n",
        "\n",
        "test_data = pd.concat([data_1, data_2])\n",
        "test_scaler = [MinMaxScaler(feature_range=(-1,1)).fit(test_data.to_numpy().T[i].reshape(-1, 1)) for i in range(test_data.shape[-1])]\n",
        "\n",
        "# scalers_1 = [MinMaxScaler(feature_range=(-1,1)).fit(data_1.to_numpy().T[i].reshape(-1, 1)) for i in range(data_1.shape[-1])]\n",
        "# scalers_2 = [MinMaxScaler(feature_range=(-1,1)).fit(data_2.to_numpy().T[i].reshape(-1, 1)) for i in range(data_2.shape[-1])]\n",
        "\n",
        "data_1_scaled = np.array([scaler.transform(data_1.to_numpy().T[i].reshape(-1, 1)) for i, scaler in enumerate(test_scaler)]).squeeze()\n",
        "data_2_scaled = np.array([scaler.transform(data_2.to_numpy().T[i].reshape(-1, 1)) for i, scaler in enumerate(test_scaler)]).squeeze()"
      ],
      "metadata": {
        "id": "SzNlD5xCcwl4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create a function to split the datasets into two week windows\n",
        "timesteps_input = 12*24  # 12 five min intervals * 4 hours \n",
        "timesteps_prediction = 12*4  # 12 five min intervals * 2 hours\n",
        "\n",
        "def create_dataset(dataset, steps_in=timesteps_input, steps_pred=timesteps_prediction):\n",
        "    \"\"\"\n",
        "    Function which creates two week chunks of x_train data, and a single\n",
        "    value for y_train.\n",
        "    \"\"\"\n",
        "    X_hist, X_pred, y = [], [], []\n",
        "    print(dataset.shape)\n",
        "    for i in tqdm(range(dataset.shape[1])):\n",
        "        target_val_start = i + steps_in\n",
        "        target_val_end = target_val_start + steps_pred\n",
        "        if target_val_end >= dataset.shape[1]:\n",
        "            break\n",
        "        feature_chunk, meteoblue_pred, target = dataset[:-5, i:target_val_start], \\\n",
        "                                                dataset[-5:, target_val_start:target_val_end], \\\n",
        "                                                dataset[4, target_val_start:target_val_end]\n",
        "        X_hist.append(feature_chunk)\n",
        "        X_pred.append(meteoblue_pred)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X_hist), np.array(X_pred), np.array(y)"
      ],
      "metadata": {
        "id": "ZfCNC6cNcwl6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# create training data for NN\n",
        "X_hist_1, X_pred_1, y_1 = create_dataset(data_1_scaled)\n",
        "X_hist_2, X_pred_2, y_2 = create_dataset(data_2_scaled)"
      ],
      "metadata": {
        "id": "zcHHqiCCcwl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_hist_1.shape)\n",
        "print(X_pred_1.shape)\n",
        "print(y_1.shape)\n",
        "print(X_hist_2.shape)\n",
        "print(X_pred_2.shape)\n",
        "print(y_2.shape)"
      ],
      "metadata": {
        "id": "vE1cJ1fKfCQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create LSTM Model"
      ],
      "metadata": {
        "id": "T9FM8kGKejQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input needs to be [samples, timesteps, features]\n",
        "units_1 = 64\n",
        "units_2 = 96\n",
        "units_3 = 64\n",
        "units_4 = 64\n",
        "units_dense = 64\n",
        "dropout = 0.0\n",
        "epochs = 40\n",
        "# val_split = 0.15\n",
        "optimizer = 'adam'\n",
        "loss='mse'\n",
        "file_name = 'wg_multi_4hour_branches_mb_forecast_L64rs_L96rs_L64_D64_C_D64_hard_validation_mse_changed_split'"
      ],
      "metadata": {
        "id": "JmysO06fRJVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiple inputs from https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/\n",
        "input_hist = Input(shape=(X_hist_2.shape[1], X_hist_2.shape[2]))\n",
        "input_pred = Input(shape=(X_pred_2.shape[1], X_pred_2.shape[2]))\n",
        "\n",
        "# branch for historical wonderground data\n",
        "x = LSTM(units_1, dropout=dropout, return_sequences=True)(input_hist)\n",
        "x = LSTM(units_2, return_sequences=True)(x)\n",
        "x = LSTM(units_3)(x)\n",
        "x = Dense(units_4, activation='relu')(x)\n",
        "x = Model(inputs=input_hist, outputs=x)\n",
        "\n",
        "# branch for meteoblue forecast data\n",
        "y = LSTM(units_1, dropout=dropout, return_sequences=True)(input_pred)\n",
        "y = LSTM(units_2, return_sequences=True)(y)\n",
        "y = LSTM(units_3)(y)\n",
        "y = Dense(units_4, activation='relu')(y)\n",
        "y = Model(inputs=input_pred, outputs=y)\n",
        "\n",
        "# combine branches\n",
        "combined = concatenate([x.output, y.output])\n",
        "z = Dense(units_dense, activation='relu')(combined)\n",
        "# z = LSTM(64)(z)\n",
        "z = Dense(y_2.shape[1])(z)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "\n",
        "checkpoint_filepath = 'drive/MyDrive/data/{}_cp'.format(file_name)\n",
        "mcp_save = keras.callbacks.ModelCheckpoint(checkpoint_filepath, save_best_only=True, monitor='val_loss', mode='min')\n",
        "early_stopping = EarlyStopping(patience=8, monitor='val_loss', restore_best_weights=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss)\n",
        "print(model.summary())\n",
        "\n",
        "history = model.fit([X_hist_2, X_pred_2], y_2, validation_data=([X_hist_1, X_pred_1], y_1), batch_size=300,\n",
        "                    epochs=epochs, verbose=1, callbacks=[early_stopping])\n",
        "\n",
        "# batchsize größer"
      ],
      "metadata": {
        "id": "rYUQJ_vxeqrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epoch = np.arange(1, len(val_loss)+1, 1)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "plt.plot(epoch,loss)\n",
        "plt.plot(epoch,val_loss)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Z32Y8CbjZTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('drive/MyDrive/data/{}'.format(file_name))"
      ],
      "metadata": {
        "id": "qbiI5z0yqPOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('drive/MyDrive/data/{}'.format(file_name))"
      ],
      "metadata": {
        "id": "AwuoU67VxAin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "-WNPCrqbkf8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot relevant times (7am - 2pm)\n",
        "\n",
        "while True:\n",
        "    r = random.randrange(0, len(X_hist_1))\n",
        "    x_hist = X_hist_1[r]\n",
        "    x_hist_exp = np.expand_dims(x_hist, 0)\n",
        "    x_pred = X_pred_1[r]\n",
        "    x_pred_exp = np.expand_dims(x_pred, 0)\n",
        "    expected = y_1[r]\n",
        "    prediction = model([x_hist_exp, x_pred_exp], training=False)[0]\n",
        "\n",
        "    x_hist_real = np.array([scaler.inverse_transform(x_hist[i].reshape(-1, 1)) for i, scaler in enumerate(test_scaler[:-5])])\n",
        "    x_pred_real = np.array([scaler.inverse_transform(x_pred[i].reshape(-1, 1)) for i, scaler in enumerate(test_scaler[-5:])])\n",
        "    expected_real = test_scaler[4].inverse_transform(expected.reshape(-1, 1))\n",
        "    prediction_real = test_scaler[4].inverse_transform(prediction.numpy().reshape(-1, 1))\n",
        "\n",
        "    if int(x_hist_real[2][-1]) in range(7, 12, 1):\n",
        "        break\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 9))\n",
        "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"}\n",
        "\n",
        "axes[0].set_title('Scaled Values - Date: {:02d}.{:02d}. {:02d}:{:02d}'.format(int(x_hist_real[0][-1]), int(x_hist_real[1][-1]), \n",
        "                                                       int(x_hist_real[2][-1]), int(x_hist_real[3][-1])))\n",
        "axes[0].plot(expected, label='Expected')\n",
        "axes[0].plot(prediction, label='Prediction')\n",
        "axes[0].plot(x_pred[0], label='MeteoBlue PV-Forecast')\n",
        "# axes[0].plot(x_pred[-1], label='MeteoBlue Cloud-Forecast')\n",
        "axes[0].legend()\n",
        "axes[0].grid()\n",
        "\n",
        "print('MSE MeteoBlue: {:.4f}'.format(mean_squared_error(expected, x_pred[0])))\n",
        "print('MSE Prediction: {:.4f}'.format(mean_squared_error(expected, prediction)))\n",
        "print('\\n')\n",
        "\n",
        "axes[1].set_title('Real Values - Date: {:02d}.{:02d}. {:02d}:{:02d}'.format(int(x_hist_real[0][-1]), int(x_hist_real[1][-1]), \n",
        "                                                       int(x_hist_real[2][-1]), int(x_hist_real[3][-1])))\n",
        "axes[1].plot(expected_real, label='Expected')\n",
        "axes[1].plot(prediction_real, label='Prediction')\n",
        "axes[1].plot(x_pred_real[0], label='MeteoBlue PV-Forecast')\n",
        "# axes[1].plot(x_pred_real[-1], label='MeteoBlue Cloud-Forecast')\n",
        "axes[1].legend()\n",
        "axes[1].grid()\n",
        "\n",
        "print('Real MSE MeteoBlue: {:.4f}'.format(mean_squared_error(expected_real, x_pred_real[0])))\n",
        "print('Real MSE Prediction: {:.4f}'.format(mean_squared_error(expected_real, prediction_real)))\n",
        "print('\\n')\n",
        "\n",
        "print('Meteoblue max/min: {}/{}'.format(max(x_pred_real[0]), min(x_pred_real[0])))\n",
        "print('Expected max/min: {}/{}'.format(max(expected_real), min(expected_real)))"
      ],
      "metadata": {
        "id": "wmw2Kevc-mJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mZOzmN_DwDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73-iYOEIIEEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}